{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mfdYGBohImm"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "import locale\n",
        "locale.getpreferredenoding = lambda: \"UTF:8\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !export LC_ALL=en_US.UTF-8\n",
        "# locale.getpreferredenoding = lambda: \"ANSI_X3.4-1968\"\n"
      ],
      "metadata": {
        "id": "anTFr4cblVgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "f3557bE5jgEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tloen/alpaca-lora.git"
      ],
      "metadata": {
        "id": "lFGcqROEj1V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd alpaca-lora"
      ],
      "metadata": {
        "id": "BnZ8s0RFj7ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -u pip\n",
        "!pip install -r requirements.txt\n",
        "!pip install torch==2.0.0\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "87fwl_ABkEDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "import transformers\n",
        "import textwrap\n",
        "from transformers import LlamaTokenizer, GenerationConfig, LlamaForCausalLM\n",
        "from transformers.generation.utils import GreedySearchDecoderOnlyOutput"
      ],
      "metadata": {
        "id": "CUUap9YEkndt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "Bqt6dVDNl0q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")"
      ],
      "metadata": {
        "id": "Zeh58k0kni1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\", load_in_8bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "rieScwNBn0bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(model, \"tloen/alpaca-lora-7b\", torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "3gzypM1uoPlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # unwind broken decapoda-research config\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
        "    model.config.bos_token_id = 1\n",
        "    model.config.eos_token_id = 2"
      ],
      "metadata": {
        "id": "k2ImHJyyo40N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.eval()\n",
        "model = torch.compile(model)"
      ],
      "metadata": {
        "id": "1qmJ73OiqA3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = f\"\"\"\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request\n",
        "\n",
        "### Instruction:\n",
        "[Instruction]\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def create_prompt(prompt):\n",
        "  final_prompt = f\"\"\"\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request\n",
        "\n",
        "### Instruction:\n",
        "[{prompt}]\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "  return final_prompt"
      ],
      "metadata": {
        "id": "Ffmt70-QgK43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = create_prompt(\"berikan rumus integral\")"
      ],
      "metadata": {
        "id": "CuG95i54gWVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer(final_prompt, return_tensors=\"pt\" )"
      ],
      "metadata": {
        "id": "jZW4YXx9gjr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = encoding[\"input_ids\"].to(DEVICE)"
      ],
      "metadata": {
        "id": "wG3dQlY0goT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(temperature=0.1, top_p=0.75, repetition_penalty=1.1)"
      ],
      "metadata": {
        "id": "wWvpB3R1gq1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    output = model.generate(input_ids=input_ids, \n",
        "                            generation_config=generation_config, \n",
        "                            return_dict_in_generate=True, \n",
        "                            output_scores=True,\n",
        "                            max_new_tokens=256)"
      ],
      "metadata": {
        "id": "gBGpFtQbgsm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(output.sequences[0])\n",
        "\n",
        "print(response.split(\"### Document Classs:\")[-1].strip())"
      ],
      "metadata": {
        "id": "q0S7U8p_qajo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = input(\"auth token : \")\n",
        "ngrok.set_auth_token(token)\n",
        "# start ngrok tunnel\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "IoCkq_7XiWYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !export LC_ALL=en_US.UTF-8\n",
        "# !pip install pyngrok\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/myendpoint', methods=['POST'])\n",
        "def prompt():\n",
        "    data = request.get_json()\n",
        "    print(\"data:\",data)\n",
        "    prompt = create_prompt(data['prompt'])\n",
        "    print(\"prompt:\",prompt)\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\" )\n",
        "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
        "    generation_config = GenerationConfig(temperature=0.1, top_p=0.75, repetition_penalty=1.1)\n",
        "    with torch.inference_mode():\n",
        "      output = model.generate(input_ids=input_ids, \n",
        "                              generation_config=generation_config, \n",
        "                              return_dict_in_generate=True, \n",
        "                              output_scores=True,\n",
        "                              max_new_tokens=256)\n",
        "    # Do something with the data here\n",
        "    output2 = tokenizer.decode(output.sequences[0])\n",
        "    response_index = output2.split(\"### Document Classs:\")[-1].strip().find(\"### Response:\")\n",
        "    res = output2[response_index+len(\"### Response:\"):].strip()\n",
        "\n",
        "    response = {'response':res}\n",
        "\n",
        "    print(response)\n",
        "    return jsonify(response)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    port = 5000\n",
        "    print('Public URL:', public_url)\n",
        "    app.run(port=port)\n"
      ],
      "metadata": {
        "id": "B_E29CgOh1E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(output.sequences[0])\n",
        "\n",
        "print(response.split(\"### Document Classs:\")[-1].strip())"
      ],
      "metadata": {
        "id": "0oyh9-sRgwQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = f\"\"\"\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request\n",
        "\n",
        "### Instruction:\n",
        "[Instruction]\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "CLASSIFICATION_TEMPLATE = f\"\"\"\n",
        "Below is the complete content of a document. Can you identify which category it is likely to be belonging to?.\n",
        "The possible categories are given below.\n",
        "\n",
        "1. Insurance claim request\n",
        "2. Leave of absense letter \n",
        "3. Invitation letter\n",
        "\n",
        "Write a response which identifies the document to any one of the above classes\n",
        "\n",
        "### Content:\n",
        "\n",
        "From,\n",
        "Sherlock Holmes\n",
        "221B Baker Street\n",
        "\n",
        "Date: 01/04/2023\n",
        "\n",
        "To,\n",
        "General Insurance Company\n",
        "Lombard Street London\n",
        "United Kingdom\n",
        "\n",
        "Dear sir/madam;\n",
        "\n",
        "Re: Policy No - P-123-456-789\n",
        "\n",
        "I am writing this letter in response to your letter on 30/03/2023, offering £400. to fix my claim. I don’t think that this is an appropriate sum.\n",
        "\n",
        "I request you to check out my case. If I do not get a reasonable offer, I will have no alternative but to refer my case to the Financial Ombudsman Service.\n",
        "\n",
        "\n",
        "I will review forward to get your offer within the following 14 days.\n",
        "\n",
        "Yours sincerely/faithfully;\n",
        "\n",
        "Sign and print your name.\n",
        "\n",
        "### Document Classs:\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "QUERY_TEMPLATE = f\"\"\"\n",
        "\n",
        "Based on the tabular contents provided below, can you identify the total premium paid. Output the total premium paid only.\n",
        "\n",
        "### Content:\n",
        "\n",
        "### Response:\n",
        "\n",
        " \"\"\"\n",
        "\n",
        "\n",
        "def create_prompt(instruction: str) -> str:\n",
        "    return PROMPT_TEMPLATE.replace(\"[Instruction]\", instruction)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fsHTLztzqhB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = CLASSIFICATION_TEMPLATE\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\" )\n"
      ],
      "metadata": {
        "id": "K8atZebKsUeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = encoding[\"input_ids\"].to(DEVICE)"
      ],
      "metadata": {
        "id": "Ic99tjOasyh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(temperature=0.1, top_p=0.75, repetition_penalty=1.1)"
      ],
      "metadata": {
        "id": "yDWqResEs14V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    output = model.generate(input_ids=input_ids, \n",
        "                            generation_config=generation_config, \n",
        "                            return_dict_in_generate=True, \n",
        "                            output_scores=True,\n",
        "                            max_new_tokens=256)"
      ],
      "metadata": {
        "id": "Hq0Fnp7ltpix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(output.sequences[0])\n",
        "\n",
        "print(response.split(\"### Document Classs:\")[-1].strip())"
      ],
      "metadata": {
        "id": "YBVnTRT_uafQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}